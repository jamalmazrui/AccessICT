3rd Annual State of Accessibility Report (SOAR)


[]

Foreword

After months of planning, collecting, and analyzing data, looking for trends, and compiling results, I am excited to present the Third Annual State of Accessibility Report (SOAR). Every year, we see companies embracing accessibility, talking about it, and raising awareness. Accessibility is having a moment.

But at the same time, we felt a certain disconnect between the excitement around accessibility and what we were seeing in reality. Understanding this disconnect is what inspired us to create SOAR three years ago, and drives us to continue the work today to uncover trends, measure progress, and identify areas for improvement.

Many people with disabilities continue to face frustration with the accessibility of the applications they use on a daily basis. We had to know, is this awareness actually achieving tangible results? Do we have a positive or negative trend line? In our first SOAR report, we started compiling data on the Alexa Top 100 websites², to determine a baseline, and now I’m happy to say that we have some answers with three years of data to analyze.

In previous years, we’ve partnered with other organizations to flesh out our report, because one year’s worth of data wasn’t enough to really understand where the industry stands. But now we are seeing clear patterns in the data. Although some of our work used automated methods to gather data, it is only possible to identify 30% of accessibility issues using automation. Therefore, we also used manual testing of every single digital property we looked at in order to have full confidence in our results.

In terms of the dataset we used, we focused on the most popular digital products rather than going for a high volume of properties. The state of accessibility

by Joe Devon, Co-Founder of Diamond and Chair of the GAAD Foundation^(^([1]))

is poor enough that trends in accessibility can hide in too much data.

If change is going to happen, it will start at the top. And if things get better at the top, it will encourage change in the rest of the ecosystem. Additionally, when you consider the Pareto Principle or the 80/20 rule as it’s known, you will capture about 80% of the traffic by reaching the top 20% of active digital products. I suspect when it comes to the web and mobile apps, it’s more like a 90/10 rule.

But What About Mobile? 

While websites are very important, the reality is we needed to add another element to the SOAR report. Mobile apps! I am beyond thrilled to announce this report includes an analysis of the top Android and iOS mobile apps. Although we are in year one of the mobile app analysis, we’ve already learned a lot. This is important work because WCAG 2.1^(^([2])) added mobile-specific requirements such as the ability to change screen orientation.

We got excited as the data came in from the free apps because the numbers looked great. But then we were let down when the paid app test results came in. They were notably worse.

For some tests, iOS fared better than Android. And in other tests, it was the reverse. I would say Android probably gets the edge, but not by much. I don’t want to reveal all in this Foreword, so I encourage you to dive in.

I hope you enjoy this report, learn from it, take action, and give us feedback.

[]

Table of Contents

Foreword.................................................................. 2

But What About Mobile?

Table of Contents................................................... 3

Introduction............................................................. 5

Diamond Website Testing.................................... 6

Website Testing Methodology............................ 7

Registration Process

Login and Logout Process

Screen Reader Testing Ratings

Technology Used in Testing

Automated Testing Process

Website Testing...................................................... 8

Website Manual Screen Reader Testing Results

Website Automated Testing Results

Website Testing Summary

Diamond Mobile App Testing............................. 12

Mobile Testing Methodology.............................. 13

Technology Used in Mobile Testing

Manual Testing

Mobile Screen Reader Results — Free Apps...14

Mobile Manual Results — Free Apps................ 15

Orientation

Text Resize

Alternative Text

Headings

Total for All Tested Elements

Mobile Manual Results — Paid Apps................17

Orientation

Text Resize

Alternative Text

Headings

Total for All Tested Elements

Mobile Screen Reader Results — App Feature...19

Mobile App Testing Summary........................... 20

Conclusion..............................................................22

Appendix A: WCAG 2.1 Success Criteria.........23

Infographic Poster................................................ 25

Contact Info............................................................26

[]

Introduction

For this year’s State of Accessibility Report (SOAR), Diamond tested accessibility conformity for websites as done in 2019 and 2020. For the first time, this report includes the testing of mobile apps.

 

As in previous years, the Diamond team tested the registration and login / logout processes of the Alexa Top 100 websites using Voiceover, NVDA, and JAWS. The team also tested criteria from the Web Content Accessibility Guidelines (WCAG) 2.1 using axe, an automated testing tool.

 

For Diamond’s first-ever testing of mobile apps, the process involved testing 20 of the top apps as of October 2021 from the Apple App Store and Google Play Store. The team rigorously tested using both a manual process and screen reader technology.

 

The reason we measure the top apps is due to the rest of the industry taking their cues from the top. It so happens many of the companies at the top are leaders that many tend to watch and follow.

[]

[] Diamond Website Testing 

Website Testing Methodology

In defining the scope of the websites to test, Diamond determined that if the tester could not complete registration using a screen reader, the tester would not do any further testing on the website. If the registration was inaccessible, the entire authentication process was deemed inaccessible.

 

Websites sharing a registration process with their parent company only underwent testing once. For example, the testing results do not include YouTube registration because YouTube forwarded the user to Google for the registration process. Also, this study only included English websites from the Alexa Top 100 list in 2021.

Registration Process 

The registration process on most websites typically contained the following steps:

 

1.   Find a link or button to start the registration.

2.   Fill out the form.

3.   Submit the form.

4.   Complete a confirmation process. (Not all             Most of the failures recorded for the registration process occurred in Step 1 or 2. If a screen reader could not access registration, the site received a failing grade. Similarly, if the page contained any forms requiring a CAPTCHA that had no audio alternative, the site received a failing grade.

Login and Logout Process

After successfully completing the registration process, the testers analyzed the login and logout processes. If the tester could not access form elements (inputs and buttons) using a screen reader, the site received a failing grade for that category.

Screen Reader Testing Ratings 

The testers used the following guidelines to determine whether a site is considered accessible for screen reader users:

•                     Accessible: The user can register, sign in, and sign out without issue.

•                     Accessible with Difficulty:  The user can complete the tasks, but only after working through challenging processes. For example, selecting a login button or link and then needing

to search for form elements to proceed because   websites required this.) the button or link didn’t have focus.

•                     Inaccessible: The user cannot register, sign in, or sign out.

Technology Used in Testing 

Screen readers, browsers, and operating systems:

•     VoiceOver on Safari with macOS Big Sur

•     NVDA on Firefox with Windows 10

•     JAWS on Internet Explorer 11 with Windows 10

Automated Testing Process 

As part of Diamond’s rigorous testing process, testers ran automated tests on each site to complement and validate manual and functional testing. The testers used the axe add-on for Chrome to check for the following:

•    A valid document “lang” attribute

•    Sufficient color contrast

•    Input field labels

•    Proper use of ARIA

•    Alternative text for images

•    Buttons and links have text labels

Website []Testing

Website Manual Screen Reader Testing Results 

Many significant factors influenced Diamond’s 2021 SOAR results. The COVID-19 pandemic changed the way most people interact with the world. This led to a dramatic change in websites listed in the Alexa Top 100 websites compared to those tested in 2019 and 2020:

•    There were 31 new websites listed in 2021 that were not on the Alexa Top 100 in 2019 or 2020.

•    Only 60% of the tested websites on the Alexa

Top 100 list in 2019 remained on the list in 2021.

 

A closer look at the most visited websites added in 2021 shows the pandemic has influenced online behavior. The inclusion of websites such as WeTransfer, Zoom, and Slack on the Alexa Top 100 shows the increase of productivity tools required to interact remotely. Moreover, the addition of websites such as USPS, FedEx, and UPS show the continued reliance on delivery services to support physical interaction between people in different locations. In 2021, Diamond saw a significant change in the accessibility of websites:

•    62% were accessible

•    9% were accessible with difficulty    

•    29% remained inaccessible

This is a notable increase in websites that are accessible without errors since 2020. When you compare it to 2019, this increase is more apparent. 

[]

with Difficulty

Screen Reader Testing of the Alexa Top 100 Websites

The trend towards improved accessibility is hard to deny. Here’s what testing revealed for the 31 websites on the 2021 Alexa Top 100 website list that were not listed in 2019 or 2020:

•    61% were accessible

•    7% were accessible with difficulty

•    32% were inaccessible

This tracks closely with the overall results of 2021. By comparison, of the 29 websites that were included on the list in 2019 and 2020, but not in 2021:

•    21% were accessible

•    24% were accessible with difficulty

•    55% were inaccessible

After three years of website screen reader testing, Diamond has identified a positive trend. In 2019, only 29% of websites listed on the Alexa Top 100 were accessible on all three screen reader platforms.

[]

Website Automated Testing Results 

Diamond performed automated testing on the registration pages of the Alexa Top 100 websites to evaluate their overall accessibility. If the registration page was not separate (i.e., appeared in a modal over the homepage), then the team tested the homepage.

Here are the results of this testing:

 

•                     Valid document “lang” attribute: None had missing lang attributes — all pages passed!

•                     Sufficient color  contrast: 67% of pages had errors for an average of 22.7 contrast failures per page.

•                     Input field labels: 11% of pages had errors for an average of 0.2 labels missing per page.

•                     Proper use of ARIA: 97% of pages had errors for an average of 11.6 instances per page.

•                     Alternative text for images: 24% of pages had errors for an average of 2.7 instances per page.

•                     Buttons and links have text labels: 43% of pages had errors for an average of 6 instances per page.

[]of Websites Passed Document Language

Attribute Test

The automated testing showed a wide range of results. The error rate for the document “lang” attribute and input field labels was quite low. This is excellent. And while alternative text for images received a good score, there is much room for improvement.

[]

        Labels                 Images               Have Text                ARIA                  Contrast

Labels-

Average Failures per Page Automated Testing of the Alexa Top 100 Websites

Unfortunately, proper ARIA use received the lowest score. This is not a surprise because it’s a known problem in the industry. The results also showed a low success rate for button and text labels and a lower success rate for sufficient color contrast. This, too, has been a common accessibility issue.

Website Testing Summary 

The changes since 2019 show positive steps towards screen reader accessibility. While it will take some time for it to trickle down to the rest of the web, the results of Diamond’s website testing validate that promising change is happening at the top. Our 2021 data shows that the number of screen reader accessible websites has grown since 2019, while the number of inaccessible websites has decreased.

[]

The drop in inaccessible or accessible with difficulty homepages and the rise in accessible homepages does not mean that all the websites that were inaccessible in 2019 are now accessible. That’s because a closer look at the data tells a different story.

 

Out of the 29 websites that Diamond tested on the Alexa Top 100 list in 2019 and 2020 that did not make the testing list in 2021:

•    55% were inaccessible

•    21% were fully accessible

•    24% were accessible with difficulty

The screen reader testing of the Alexa Top 100 websites showed significant improvements over the last two years. Diamond notes that the makeup of the Alexa Top 100 websites changed greatly due to the pandemic, which affected the public’s digital needs.

Automated accessibility testing of the same websites indicated positive and negative trends in the industry. For example, defining a document language had zero errors, while proper use of ARIA had a 97% error rate.

[]

Diamond Mobile App Testing 

[]

Diamond added mobile accessibility testing to this report due to the growing use of mobile devices. Although there have been many studies about web accessibility, there aren’t comparable studies for mobile accessibility.

Mobile Testing Methodology

The selected mobile apps for testing included the top 20 free and top 20 paid apps for iOS from the Apple App Store. The same goes for Android, as testing covered the top 20 free and top 20 paid apps in the Google Play Store.

Diamond selected the top 20 apps from both the Apple App Store and Google Play Store in October of 2021 that met the following criteria:

•                     Not dependent on a secondary device, such as a smartwatch or television.

•                     Not redundant to other apps being tested from the same developer, such as the paid children’s apps from Toca Boca, which had many versions of the same app on the top 20 Android paid apps.

When an app in the top 20 free and paid apps became ineligible for testing, Diamond selected the next app that met the criteria.

Screen Reader Testing Ratings

The testers used the same guidelines as those used in website testing to determine whether a site is considered accessible for screen reader users:

•                     Accessible: The user can register, sign in, and sign out without issue.

•                     Accessible with Difficulty:  The user can complete the tasks, but only after working through challenging processes. For example, selecting a login button or link and then needing to search for form elements to proceed because the button or link didn’t have focus.  

•                     Inaccessible: The user cannot register, sign in, or sign out.

For iOS and Android paid apps, paying for the app served as registration for most of the apps Diamond tested on the top 20 lists. Thus, there are no results for paid apps on this process.

Technology Used in Mobile Testing

•  VoiceOver on iPhone 12 with iOS 15

•   TalkBack on Samsung Galaxy 6 with Android 11 

Manual Testing

The purpose of manual testing on the free apps’ registration and home screens and paid apps’ home screens was to determine whether the apps met the following criteria:

 

•                     Does the app change orientation after rotating the device?

•                     Does the text resize in the app based on the device’s settings?

•                     If present, do informative images have acceptable alternative text?

•                     If present, does the programming ensure screen readers identify headings properly?

All apps received a pass/fail grade based on their ability to meet the criteria.

The process also included testing the main features of the top 20 free and top 20 paid apps using screen reader technology with each receiving a pass/fail grade.

Mobile Screen Reader Results — Free Apps

Diamond performed screen reader testing on the registration and login / logout processes for the top 20 free apps on iOS and Android for a total of 40 apps. Here are the results.

[]

Summary: Both mobile platforms scored fairly well, with Android edging out iOS. Using a screen reader, testers were able to register, login, and logout successfully for nearly three-quarters of the free mobile apps.

[]

Free Mobile App Testing with Screen Readers

Mobile Manual Results — Free Apps

Diamond performed manual testing on the registration and home screens of the top 20 free apps on iOS and Android for a total of 40 apps.

Here are the results of that testing.

Orientation

Do the screens of the app change orientation after rotating the device?

  ------------------------ ------------------------
  FREE iOS APPS            FREE ANDROID APPS
  Registration screen      Registration screen
  Passed: 26%              Passed: 25%
  Failed: 74%              Failed: 75%
  Home screen              Home screen
  Passed: 30%              Passed: 25%
  Failed: 70%              Failed: 75%
  Total for orientation    Total for orientation
  requirement              requirement
  Passed: 28%              Passed: 25%
  Failed: 72%              Failed: 75%
  ------------------------ ------------------------

Summary: iOS and Android apps were similar in their low compliance to the accessibility requirement for orientation. Although orientation is a newer standard (WCAG 2.1) and lower compliance rates are to be expected, these results still disappoint.

Meeting this criterion takes pre-design work and planning for accessibility to support both orientations. It requires layout adjustments. It’s a stark reminder of the importance of designing properties accessible from the start of the product life cycle — also known as a “shift-left” approach — as retrofitting is more complex and requires far more time and resources.

Text Resize

Does the text in the app resize based on the user’s settings?

  ------------------------ ------------------------
  FREE iOS APPS            FREE ANDROID APPS
  Registration screen      Registration screen
  Passed: 15.79%           Passed: 55%
  Failed: 84.21%           Failed: 45%
  Home screen              Home screen
  Passed: 40%              Passed: 50%
  Failed: 60%              Failed: 50%
  Total for text resize    Total for text resize
  requirement              requirement
  Passed: 28.21%           Passed: 52.5%
  Failed: 71.79%           Failed: 47.5%
  ------------------------ ------------------------

Summary: Android free apps performed better than iOS in the text resize criterion. The low percentages here disappoint because this is much easier for developers to accomplish than in prior operating system versions. By default, Android development supports text resize. iOS requires developers to use built-in fonts or modify their custom fonts to accommodate resizing.

Alternative Text

If present, do informative images have appropriate alternative text?

  ------------------------ ------------------------
  FREE iOS APPS            FREE ANDROID APPS
  Registration screen      Registration screen
  Passed: 95%              Passed: 90%
  Failed: 5%               Failed: 10%
  Home screen              Home screen
  Passed: 70%              Passed: 65%
  Failed: 30%              Failed: 35%
  Total for alternative    Total for alternative
  text requirement         text requirement
  Passed: 82.5%            Passed: 77.5%
  Failed: 17.5%            Failed: 22.5%
  ------------------------ ------------------------

Summary: iOS and Android apps mostly met the requirements for appropriate alternative text.

Considering this is one of the most fundamental requirements, it’s not surprising that there’s a high level of compliance.

Headings

If present, does the programming ensure screen readers identify headings properly?

+-----------------------------------------------+-----------------------------------+
| FREE iOS APPS                                 | FREE ANDROID APPS                 |
+-----------------------------------------------+-----------------------------------+
| Registration screen                           | Registration screen               |
+-----------------------------------------------+-----------------------------------+
| Passed: 100%                                  | Passed: 95%                       |
+-----------------------------------------------+-----------------------------------+
| Failed: 0%                                    | Failed: 5%                        |
+-----------------------------------------------+-----------------------------------+
| Home screen                                   | Home screen                       |
+-----------------------------------------------+-----------------------------------+
| Passed: 95%                                   | Passed: 95%                       |
+-----------------------------------------------+-----------------------------------+
| Failed: 5%                                    | Failed: 5%                        |
+-----------------------------------------------+-----------------------------------+
| Total for headings requirement Passed: 97.5%  | Total for headings requirement    |
|                                               |                                   |
| Failed: 2.5%                                  | Passed: 95% Failed: 5%            |
+-----------------------------------------------+-----------------------------------+

Total for All Tested Elements 

  ----------------- --------------------
  FREE iOS APPS     FREE ANDROID APPS
  Passed: 59.10%    Passed: 62.50%
  Failed: 40.90%    Failed: 37.50%
  ----------------- --------------------

Summary: iOS and Android performed the highest when testing for the headings criteria.

This may be due to the following: • Headings are a fundamental requirement that have been available in the mobile development toolkits for a few years.

•                     Substantial use of web views where creating headings is easier.

•                     Popular apps developed by companies that have an understanding of improved accessibility based on their larger user base and community feedback.

Mobile Manual Results — Paid Apps

Diamond performed manual testing on the home screens of the top 20 paid apps on iOS and Android totaling 40 apps. Note that the registration screen did not need testing because the payment serves as registration for paid apps.

Orientation

Do the screens of the app change orientation after rotating the device?

  ---------------- --------------------
  PAID iOS APPS    PAID ANDROID APPS
  Passed: 30%      Passed: 55%
  Failed: 70%      Failed: 45%
  ---------------- --------------------

Summary: Unlike the free app test results, Android performed better in this category than iOS.

Text Resize

Does the text in the app resize based on the user’s settings?

  ---------------- --------------------
  PAID iOS APPS    PAID ANDROID APPS
  Passed: 15%      Passed: 25%
  Failed: 85%      Failed: 75%
  ---------------- --------------------

Summary: Both iOS and Android results were lower for the paid apps than for the free apps. Paid apps tend to have fewer users than free apps. Because of this, they may not have as much accessibility testing and user feedback as the free apps with wider user bases.

Alternative Text

If present, do informative images have appropriate alternative text?

  ---------------- --------------------
  PAID iOS APPS    PAID ANDROID APPS
  Passed: 45%      Passed: 25%
  Failed: 55%      Failed: 75%
  ---------------- --------------------

Summary: Surprisingly, both iOS and Android paid apps performed poorly for alternative text.

Moreover, the results are considerably lower than those for the free apps. These results are disappointing considering this is a largely known criterion. And it can be tested to a certain extent with automated tools.

Headings

If present, does the programming ensure screen readers identify headings properly?

  ---------------- --------------------
  PAID iOS APPS    PAID ANDROID APPS
  Passed: 50%      Passed: 10%
  Failed: 50%      Failed: 90%
  ---------------- --------------------

Summary: These results show iOS performing far better than Android. Both performed worse for headings in paid apps than they did in the free apps. The process for implementing headings has been supported in iOS development for years and it’s well-documented.

Total for All Tested Elements 

The combined average results of Orientation, Text Resizing, Alternative Text, and Headings from the paid apps.

  ---------------- --------------------
  PAID iOS APPS    PAID ANDROID APPS
  Passed: 35%      Passed: 29%
  Failed: 65%      Failed: 71%
  ---------------- --------------------

[]

Manual Testing of Paid Mobile Apps

Summary: For newer or less experienced programmers, the iOS programming interface provides tools to easily identify headings. For Android, the low pass rate may be partly due to a lack of testing for headings in the automated accessibility testing process when an app is submitted to the Google Play Store.

Mobile Screen Reader Results — App Feature

Using a screen reader, Diamond tested the main feature of both the top 20 free and top 20 paid apps for both iOS and Android for a total of 80 apps. If the tester could perform the task of the main feature without difficulty, the app received a passing grade. Otherwise, it received a failing grade.

  ---------------- --------------------
  FREE iOS APPS    FREE ANDROID APPS
  Passed: 80%      Passed: 65%
  Failed: 20%      Failed: 35%
  PAID iOS APPS    PAID ANDROID APPS
  Passed: 10%      Passed: 40%
  Failed: 90%      Failed: 60%
  ---------------- --------------------

Summary: The passing rate of the main feature was much higher for free apps than paid apps. The large discrepancy is somewhat unexpected, especially for iOS. But that could be because free apps tend to come from large companies. They typically have wider user bases than paid apps, and therefore are subject to more scrutiny for accessibility.

[]

Screen Reader Testing of

Main Feature of Mobile Apps

Mobile App Testing Summary

Studies show that Americans check their phones 96 times a day, or every 10 minutes on average^(^([3])). Another study found that out of four hours spent on the mobile internet, 88% of people spend that time on mobile apps^(^([4])). This is promising news for app developers, app publishers, and anyone who plans to develop an app. However, to be successful in this space, developers need to create apps that are usable, accessible, and easy to download and install for all users.

Universal design is about making sure apps are available to the maximum number of people in the maximum number of circumstances.

This includes accessibility, inclusivity, and usability. The foundation of app development is a well-designed process and universal design must be a bedrock on which that foundation rests.

For free apps, orientation and text resize scored poorly, but alternative text and headings scored highly. Paid apps, on the other hand, scored poorly across the board. When it comes to testing mobile apps’ features with a screen reader, both iOS and Android showed far better results for free apps than for paid apps.

The data shows that free apps put a priority on screen reader accessibility. However, they still need to address other areas such as orientation and text resizing.

Many of the mobile apps Diamond tested are those that allow users to interact with each other, socially, visually, and financially. Accessibility barriers of these apps will not only lock out a person with a disability but also limit society as a whole. Technology has made huge strides in the past decade, but if accessibility is left out, these technological advances and advantages will not reach their full potential.

Accessibility is crucial on mobile apps for the same reasons it is crucial on the web at large. As of the third quarter of 2021, 47.74% of web traffic in the U.S. originated from mobile devices. By comparison, over half of web traffic worldwide originated on mobile devices^(^([5])).

Note this percentage excludes tablets.

With over a billion people having a disability worldwide, mobile apps that aren’t fully accessible affect individual users as well as overall business traffic and trade. Given an option, a consumer will always choose a better alternative. If an app is difficult or impossible to use, the large market share of consumers, including people with disabilities, will most certainly choose a more accessible option.

In today’s economy, businesses in almost every sector place heavy importance on having users. It’s simple economics. A business with a larger user community will have a larger revenue stream. Applying universal design principles helps make that possible.

Now that our mobile testing provides a benchmark on the accessibility of mobile apps, we’ll find out what direction mobile apps move in next year’s report.

Conclusion

We had a lot of fun working on this project. It is particularly exciting that the Screen Reader testing of the Alexa Top 100 websites have improved so much since this report began. Hopefully, we will soon see a 90% and above screen reader accessibility score on the Alexa Top sites in the coming years, forcing us to revisit our testing moving forward.

We are encouraged by the results of our mobile screen reader testing of the top free apps on iOS and Android. However, it is a shame that the paid apps didn’t do as well. We are also disappointed that the WCAG 2.1 criteria that we tested performed so poorly.

Here’s hoping that accessibility will continue to SOAR and make strides by the time the next version of this report comes out.

[]

Appendix A WCAG 2.1 Success Criteria

For the Web Automated Testing of this report, the following lists the WCAG 2.1 Success Criteria tested with the axe browser extension.

A valid document ‘lang’ attribute:

3.1.1 Language of Page [Level A]

Sufficient color contrast:

1.4.3 Contrast (Minimum) [Level AA]

Input field labels:

1.3.1 Info and Relationships [Level A]

4.1.2 Name, Role, Value [Level A]

Proper use of ARIA:

4.1.2 Name, Role, Value [Level A]

1.3.1 Info and Relationships [Level A]

Alternative text for images:

1.1.1 Non-text Content [Level A]

Buttons and links have text labels:

4.1.2 Name, Role, Value [Level A]

2.4.4 Link Purpose (In Context) [Level A]

[]

#SOAR

State of Accessibility Report 

To learn more visit: diamond.la/SOAR To contact us email: a11y@diamond.la

[]

©2021 Diamond. All Rights Reserved.

------------------------------------------------------------------------

[1] GAAD Foundation ² Alexa Top Sites

[2] Web Content Accessibility Guidelines (WCAG) 2.1  

[3] Americans Check Their Phones 96 Times a Day      

[4] The Majority of Americans’ Mobile Time Spent Takes Place in Apps

[5] Percentage of mobile device website traffic in the

  United States from 1st quarter 2015 to 2nd quarter 2021
